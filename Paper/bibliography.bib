
@unpublished{brockLargeScaleGAN2019a,
  title = {Large {{Scale GAN Training}} for {{High Fidelity Natural Image Synthesis}}},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  date = {2019-02-25},
  eprint = {1809.11096},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.11096},
  urldate = {2021-05-17},
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\L35YRTBA\\Brock et al. - 2019 - Large Scale GAN Training for High Fidelity Natural.pdf;C\:\\Users\\amarm\\Zotero\\storage\\347ZCDUQ\\1809.html}
}

@online{brownleeHowVisualizeFilters2019,
  title = {How to {{Visualize Filters}} and {{Feature Maps}} in {{Convolutional Neural Networks}}},
  author = {Brownlee, Jason},
  date = {2019-05-05T19:00:37+00:00},
  url = {https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/},
  urldate = {2020-11-22},
  abstract = {Deep learning neural networks are generally opaque, meaning that although they can make useful and skillful predictions, it is not clear how or why a given prediction was made. Convolutional neural networks, have internal structures that are designed to operate upon two-dimensional image data, and as such preserve the spatial relationships for what was learned […]},
  langid = {american},
  organization = {{Machine Learning Mastery}},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\9PK7PV7I\\how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks.html}
}

@article{chatterjeeAssessmentArtAttributes2010,
  title = {The {{Assessment}} of {{Art Attributes}}},
  author = {Chatterjee, Anjan and Widick, Page and Sternschein, Rebecca and Smith, William B. and Bromberger, Bianca},
  date = {2010-07-01},
  journaltitle = {Empirical Studies of the Arts},
  shortjournal = {Empirical Studies of the Arts},
  volume = {28},
  number = {2},
  pages = {207--222},
  publisher = {{SAGE Publications Inc}},
  issn = {0276-2374},
  doi = {10.2190/EM.28.2.f},
  url = {https://doi.org/10.2190/EM.28.2.f},
  urldate = {2022-03-11},
  abstract = {Neuropsychological investigations of art production and perception have the potential to offer critical insight into the biology of visual aesthetics. Thus far, however, investigations of art production in patients have been limited to anecdotal observations and investigations of art perception are non-existent. Progress in the field is hampered by the lack of an adequate instrument to provide basic quantification of artwork attributes. Motivated by the need to move neuropsychology of art beyond the fascinating anecdote, we present the Assessment of Art Attributes (AAA). The AAA is an instrument designed to assess six formal-perceptual and six conceptual-representational attributes using 24 paintings from the Western canon. Both artistically naïve and experienced participants were given the AAA. We found high degrees of agreement in the assessment of these attributes in both groups and few differences between the groups. We expect that the AAA's componential and quantitative approach will be useful in advancing neuropsychological studies as well as any investigations in which style and content of art works need to be quantified and compared.},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\232MJUTX\\Chatterjee et al. - 2010 - The Assessment of Art Attributes.pdf}
}

@article{creswellGenerativeAdversarialNetworks2018,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  date = {2018-01},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {53--65},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2765202},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Convolutional codes,Data models,Generators,Image resolution,Machine learning,Semantics,Signal resolution,Training data},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\BYTEXN65\\Creswell et al. - 2018 - Generative Adversarial Networks An Overview.pdf;C\:\\Users\\amarm\\Zotero\\storage\\GJJZWHVZ\\8253599.html}
}

@online{GANsOneHottest,
  title = {{{GANs}}: {{One}} of the {{Hottest Topics}} in {{Machine Learning}}},
  url = {https://www.linkedin.com/pulse/gans-one-hottest-topics-machine-learning-al-gharakhanian?trk=pulse_spock-articles},
  urldate = {2020-11-22},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\PAXKAL9G\\gans-one-hottest-topics-machine-learning-al-gharakhanian.html}
}

@inproceedings{goetschalckxGANalyzeVisualDefinitions2019,
  title = {{{GANalyze}}: {{Toward Visual Definitions}} of {{Cognitive Image Properties}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Goetschalckx, Lore and Andonian, Alex and Oliva, Aude and Isola, Phillip},
  date = {2019-10},
  pages = {10},
  doi = {10.1109/ICCV.2019.00584},
  abstract = {We introduce a framework that uses Generative Adversarial Networks (GANs) to study cognitive properties like memorability. These attributes are of interest because we do not have a concrete visual definition of what they entail. What does it look like for a dog to be more memorable? GANs allow us to generate a manifold of naturallooking images with fine-grained differences in their visual attributes. By navigating this manifold in directions that increase memorability, we can visualize what it looks like for a particular generated image to become more memorable. The resulting “visual definitions” surface image properties (like “object size”) that may underlie memorability. Through behavioral experiments, we verify that our method indeed discovers image manipulations that causally affect human memory performance. We further demonstrate that the same framework can be used to analyze image aesthetics and emotional valence. ganalyze.csail.mit.edu.},
  langid = {english},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\SHY38WWU\\Goetschalckx et al. - GANalyze Toward Visual Definitions of Cognitive I.pdf}
}

@article{goetschalckxGenerativeAdversarialNetworks2021,
  title = {Generative Adversarial Networks Unlock New Methods for Cognitive Science},
  author = {Goetschalckx, Lore and Andonian, Alex and Wagemans, Johan},
  date = {2021-09},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {25},
  number = {9},
  pages = {788--801},
  issn = {13646613},
  doi = {10.1016/j.tics.2021.06.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661321001534},
  urldate = {2021-09-23},
  langid = {english},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\3SYP6YKE\\Goetschalckx et al. - 2021 - Generative adversarial networks unlock new methods.pdf}
}

@article{goldmanAestheticQualitiesAesthetic1990,
  title = {Aesthetic {{Qualities}} and {{Aesthetic Value}}},
  author = {Goldman, Alan H.},
  date = {1990},
  journaltitle = {The Journal of Philosophy},
  volume = {87},
  number = {1},
  eprint = {2026797},
  eprinttype = {jstor},
  pages = {23--37},
  publisher = {{Journal of Philosophy, Inc.}},
  issn = {0022-362X},
  doi = {10.2307/2026797},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\JZ8RKL5Q\\Goldman - 1990 - Aesthetic Qualities and Aesthetic Value.pdf}
}

@article{hertzmannVisualIndeterminacyGAN2020,
  title = {Visual {{Indeterminacy}} in {{GAN Art}}},
  author = {Hertzmann, Aaron},
  date = {2020-07},
  journaltitle = {Leonardo},
  shortjournal = {Leonardo},
  volume = {53},
  number = {4},
  pages = {424--428},
  issn = {0024-094X, 1530-9282},
  doi = {10.1162/leon_a_01930},
  url = {https://direct.mit.edu/leon/article/53/4/424-428/96926},
  urldate = {2021-09-23},
  abstract = {This paper explores visual indeterminacy as a description for artwork created with Generative Adversarial Networks (GANs). Visual indeterminacy describes images that appear to depict real scenes, but on closer examination, defy coherent spatial interpretation. GAN models seem to be predisposed to producing indeterminate images, and indeterminacy is a key feature of much modern representational art, as well as most GAN art. The author hypothesizes that indeterminacy is a consequence of a powerful-but-imperfect image synthesis model that must combine general classes of objects, scenes and textures.},
  langid = {english},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\IDIIRL6P\\Hertzmann - 2020 - Visual Indeterminacy in GAN Art.pdf}
}

@unpublished{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2018-11-26},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.07004},
  urldate = {2020-11-22},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\RE8IVB4M\\Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf;C\:\\Users\\amarm\\Zotero\\storage\\VSKCJCWC\\1611.html}
}

@article{jacobsenBridgingArtsSciences2006,
  title = {Bridging the {{Arts}} and {{Sciences}}: {{A Framework}} for the {{Psychology}} of {{Aesthetics}}},
  shorttitle = {Bridging the {{Arts}} and {{Sciences}}},
  author = {Jacobsen, Thomas},
  date = {2006-04-01},
  journaltitle = {Leonardo},
  shortjournal = {Leonardo},
  volume = {39},
  number = {2},
  pages = {155--162},
  issn = {0024-094X},
  doi = {10.1162/leon.2006.39.2.155},
  url = {https://doi.org/10.1162/leon.2006.39.2.155},
  urldate = {2021-08-07},
  abstract = {The investigation of aesthetic processing has constituted a longstanding tradition in experimental psychology, of which experimental aesthetics is the second-oldest branch. The status of this psychology of aesthetics, the science of aesthetic processing, is briefly reviewed here. Building on this heritage and drawing on a host of related scientific disciplines, a framework for a strongly interdisciplinary psychology of aesthetics is proposed. It is argued that the topic can be fruitfully approached from at least seven different perspectives, each with multiple levels of analysis: diachronia, ipsichronia, mind, body, content, person and situation. Eventually, this work may coalesce into a unified theory of aesthetic processing.},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\KERXA7RD\\Jacobsen - 2006 - Bridging the Arts and Sciences A Framework for th.pdf;C\:\\Users\\amarm\\Zotero\\storage\\SM3HAUEZ\\Bridging-the-Arts-and-Sciences-A-Framework-for-the.html}
}

@inproceedings{kongPhotoAestheticsRanking2016a,
  title = {Photo {{Aesthetics Ranking Network}} with {{Attributes}} and {{Content Adaptation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016},
  author = {Kong, Shu and Shen, Xiaohui and Lin, Zhe and Mech, Radomir and Fowlkes, Charless},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {662--679},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-46448-0_40},
  abstract = {Real-world applications could benefit from the ability to automatically generate a fine-grained ranking of photo aesthetics. However, previous methods for image aesthetics analysis have primarily focused on the coarse, binary categorization of images into high- or low-aesthetic categories. In this work, we propose to learn a deep convolutional neural network to rank photo aesthetics in which the relative ranking of photo aesthetics are directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated photo aesthetics rating problem.To train and analyze this model, we have assembled a new aesthetics and attributes database (AADB) which contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. Anonymized rater identities are recorded across images allowing us to exploit intra-rater consistency using a novel sampling strategy when computing the ranking loss of training image pairs. We show the proposed sampling strategy is very effective and robust in face of subjective judgement of image aesthetics by individuals with different aesthetic tastes. Experiments demonstrate that our unified model can generate aesthetic rankings that are more consistent with human ratings. To further validate our model, we show that by simply thresholding the estimated aesthetic scores, we are able to achieve state-or-the-art classification performance on the existing AVA dataset benchmark.},
  isbn = {978-3-319-46448-0},
  langid = {english},
  keywords = {Attribute learning,Convolutional neural network,Image aesthetics rating,Rank loss},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\R8Y9P4DB\\Kong et al. - 2016 - Photo Aesthetics Ranking Network with Attributes a.pdf}
}

@book{locherNewDirectionsAesthetics2020,
  title = {New {{Directions}} in {{Aesthetics}}, {{Creativity}} and the {{Arts}}},
  author = {Locher, Paul and Martindale, Colin and Dorfman, Leonid},
  date = {2020-02-10},
  eprint = {5j73DwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Routledge}},
  abstract = {The contributing authors to this book, all pre-eminent scholars in their fields, present their current thinking about the processes that underlie creativity and aesthetic experience. They discuss established theory and research and provide creative speculation on future problems for inquiry and new approaches to conceptualising and investigating these phenomena. The book contains many new findings and ideas never before published or new by virtue of the novel context in which they are incorporated. Thus, the chapters present both new approaches to old problem and new ideas and approaches not yet explored by leading scholars in these fields. The first part of the book is devoted to understanding the nature of the perceptual/cognitive and aesthetic processes that occur during encounters with visual art stimuli in everyday settings, in museums and while watching films. Also discussed in Part I is how cultural and anthropological approaches to the study of aesthetic responses to art contribute to our understanding about the development of a culture's artistic canon and to cross-cultural aesthetic universals. Part II presents new dimensions in the study of creativity. Two approaches to the development of a comprehensive theory of creativity are presented: Sternberg's Investment Theory of Creativity and a systems perspective of creativity based on a metaindividual world model. Also covered are the factors that contribute to cinematic creativity and a film's cinematic success, and the complex nature of the creative processes and research approaches involved in the innovative product design necessitated by the introduction of electronics in consumer products. Part III deals with the application of concepts and models from cognitive psychology to the study of music, literary meaning and the visual arts. The contributors outline a model of the cognitive processes involved in real-time listening to music, investigate what readers are doing when they read a literary text, describe what research shows about the transfer of learning from the arts to non-arts cognition and discuss the kinds of thinking skills that emerge from the study of the visual arts by high school students. In Part IV, the authors focus on the interactive contribution of observers' personalities and affect states to the creation and perception of art. The chapters include a discussion of the internal mechanisms by which personality expresses itself during the making of and the response to art; the relationship between emotion and cognition in aesthetics, in terms of the interaction of top-down and bottom-up processes across the time course of an aesthetic episode; the affective processes that take place during pretend play and their impact on the development of creativity in children and the causes and consequences of listener's intense experiences while listening to music.},
  isbn = {978-1-351-84283-9},
  langid = {english},
  pagetotal = {289},
  keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / Mental Health}
}

@article{russakovskyImageNetLargeScale2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2015-12-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {115},
  number = {3},
  pages = {211--252},
  issn = {1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  url = {https://doi.org/10.1007/s11263-015-0816-y},
  urldate = {2021-05-17},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5~years of the challenge, and propose future directions and improvements.},
  langid = {english},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\Q7MNRSDX\\Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf}
}

@article{speckerViennaArtInterest2020,
  title = {The {{Vienna Art Interest}} and {{Art Knowledge Questionnaire}} ({{VAIAK}}): {{A}} Unified and Validated Measure of Art Interest and Art Knowledge},
  shorttitle = {The {{Vienna Art Interest}} and {{Art Knowledge Questionnaire}} ({{VAIAK}})},
  author = {Specker, Eva and Forster, Michael and Brinkmann, Hanna and Boddy, Jane and Pelowski, Matthew and Rosenberg, Raphael and Leder, Helmut},
  date = {2020-05},
  journaltitle = {Psychology of Aesthetics, Creativity, and the Arts},
  volume = {14},
  number = {2},
  pages = {172--185},
  publisher = {{Educational Publishing Foundation}},
  location = {{Washington, US}},
  issn = {1931-3896},
  doi = {http://dx.doi.org.kuleuven.e-bronnen.be/10.1037/aca0000205},
  url = {http://www.proquest.com/docview/2126302824/abstract/C12886CDBA4C4974PQ/1},
  urldate = {2022-03-11},
  abstract = {Being interested in art and having knowledge about art are arguably central dimensions in art experience and two of the most important individual differences when assessing how people process or respond to art. Nonetheless, there is to date no reliable and validated measure of these dimensions. In this paper, we present the Vienna Art Interest \& Art Knowledge Questionnaire (VAIAK) as a tool for researchers to measure both art interest and knowledge of their participants in studies involving visual art. In a 3-stage validation process, we first developed the questionnaire. Second, we pretested it qualitatively and quantitatively with a sample of lay people as well as art history students and art professionals. Finally, we conducted a large-scale validation study ({$>$}600 participants) with both lay people (psychology students) and art history students, where we present evidence regarding relations to other variables and internal structure as well as the ability to discriminate between relative lay people and experts. Our results show evidence for the reliability of the VAIAK scores as well as evidence for the validity as a measure of both art interest and art knowledge in (scientific) research, while at the same time also providing important clarification and differentiation between the two dimensions. With this new questionnaire, we offer a tool for researchers in empirical aesthetics, creativity, and also applied fields to quantify one or both of the art interest and art knowledge dimensions. (PsycInfo Database Record (c) 2020 APA, all rights reserved) (Source: journal abstract)},
  isbn = {9781433893629},
  langid = {english},
  pagetotal = {172-185},
  keywords = {Aesthetics,Art (major),Creativity,Experience Level,Individual Differences,Interests,Knowledge (General) (major),Questionnaires (major),Test Construction,Test Reliability (major),Test Validity (major)},
  annotation = {(US)},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\7IU9CWNP\\Specker et al. - 2020 - The Vienna Art Interest and Art Knowledge Question.pdf}
}

@unpublished{suOGANExtremelyConcise2019,
  title = {O-{{GAN}}: {{Extremely Concise Approach}} for {{Auto-Encoding Generative Adversarial Networks}}},
  shorttitle = {O-{{GAN}}},
  author = {Su, Jianlin},
  date = {2019-03-05},
  eprint = {1903.01931},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1903.01931},
  urldate = {2020-11-22},
  abstract = {In this paper, we propose Orthogonal Generative Adversarial Networks (O-GANs). We decompose the network of discriminator orthogonally and add an extra loss into the objective of common GANs, which can enforce discriminator become an effective encoder. The same extra loss can be embedded into any kind of GANs and there is almost no increase in computation. Furthermore, we discuss the principle of our method, which is relative to the fully-exploiting of the remaining degrees of freedom of discriminator. As we know, our solution is the simplest approach to train a generative adversarial network with auto-encoding ability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\5MQ5BKXZ\\Su - 2019 - O-GAN Extremely Concise Approach for Auto-Encodin.pdf;C\:\\Users\\amarm\\Zotero\\storage\\7KAZRMZ6\\1903.html}
}

@article{wanzerExperiencingFlowViewing2020,
  title = {Experiencing Flow While Viewing Art: {{Development}} of the {{Aesthetic Experience Questionnaire}}.},
  shorttitle = {Experiencing Flow While Viewing Art},
  author = {Wanzer, Dana Linnell and Finley, Kelsey Procter and Zarian, Steven and Cortez, Noreen},
  date = {2020-02},
  journaltitle = {Psychology of Aesthetics, Creativity, and the Arts},
  shortjournal = {Psychology of Aesthetics, Creativity, and the Arts},
  volume = {14},
  number = {1},
  pages = {113--124},
  issn = {1931-390X, 1931-3896},
  doi = {10.1037/aca0000203},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/aca0000203},
  urldate = {2022-03-13},
  abstract = {Aesthetic experiences are the attitudes, perceptions, or acts of attention involved with viewing art. When the viewer is fully engaged, aesthetic experiences are comparable to flow experiences, which are optimal experiences described by people as "being in the zone" or "getting lost" in the moment. This study uses the framework of aesthetic experiences by Csikszentmihalyi and Robinson (1990) which conceptualizes aesthetic experiences as having four art-related dimensions and a fifth flow dimension. Using this conceptualization, we created the Aesthetic Experience Questionnaire (AEQ) and tested its validity in a general population of Amazon Mechanical Turk users. Results demonstrated the AEQ consists of four artistically-related dimensions (i.e., perceptual, emotional, cultural, understanding) and two flow dimensions (i.e., proximal conditions, experience). The AEQ demonstrated convergent validity with related constructs (i.e., openness to experience, inspiration, curiosity and exploration) and individuals with greater artistic experience had more intense ratings of aesthetic experiences. Overall, this study lends support to Csikszentmihalyi and Robinson’s framework of aesthetic experiences and provides a useful measure of aesthetic experiences for future studies.},
  langid = {english},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\JGCXIVVM\\Wanzer et al. - 2020 - Experiencing flow while viewing art Development o.pdf}
}

@unpublished{zhuInDomainGANInversion2020,
  title = {In-{{Domain GAN Inversion}} for {{Real Image Editing}}},
  author = {Zhu, Jiapeng and Shen, Yujun and Zhao, Deli and Zhou, Bolei},
  date = {2020-07-16},
  eprint = {2004.00049},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.00049},
  urldate = {2020-11-22},
  abstract = {Recent work has shown that a variety of semantics emerge in the latent space of Generative Adversarial Networks (GANs) when being trained to synthesize images. However, it is difficult to use these learned semantics for real image editing. A common practice of feeding a real image to a trained GAN generator is to invert it back to a latent code. However, existing inversion methods typically focus on reconstructing the target image by pixel values yet fail to land the inverted code in the semantic domain of the original latent space. As a result, the reconstructed image cannot well support semantic editing through varying the inverted code. To solve this problem, we propose an in-domain GAN inversion approach, which not only faithfully reconstructs the input image but also ensures the inverted code to be semantically meaningful for editing. We first learn a novel domain-guided encoder to project a given image to the native latent space of GANs. We then propose domain-regularized optimization by involving the encoder as a regularizer to fine-tune the code produced by the encoder and better recover the target image. Extensive experiments suggest that our inversion method achieves satisfying real image reconstruction and more importantly facilitates various image editing tasks, significantly outperforming start-of-the-arts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\ALRYRDIR\\Zhu et al. - 2020 - In-Domain GAN Inversion for Real Image Editing.pdf;C\:\\Users\\amarm\\Zotero\\storage\\DZ9BSVTS\\2004.html}
}

@unpublished{zhuUnpairedImagetoImageTranslation2020,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2020-08-24},
  eprint = {1703.10593},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.10593},
  urldate = {2020-11-22},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslash rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslash rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslash approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\amarm\\Zotero\\storage\\ANQE2RE6\\Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf;C\:\\Users\\amarm\\Zotero\\storage\\2E2M83HN\\1703.html}
}


